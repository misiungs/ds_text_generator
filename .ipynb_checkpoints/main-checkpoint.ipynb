{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HyVKeIaMmxZO"
   },
   "source": [
    "# Introduction\n",
    "In this notebook a text generator based on Agata Christie's books is developed.\n",
    "Different methods are investigated.\n",
    "First a simple n-gram language model is created.\n",
    "It is followed by RNN based on LSTMs.\n",
    "Finally GPT-2 transformer is fine tuned.\n",
    "# Import libraries\n",
    "We start with the import of libraries that are necessary for our n-gram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "VQ25vBe0YEN-"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from string import punctuation\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GOBFzDnUp8vr"
   },
   "source": [
    "# Load and preprocess dataset\n",
    "We gather the books from Gutenberg project [website](https://www.gutenberg.org/ebooks/author/451).\n",
    "For each file we remove preface and comments at the end.\n",
    "Data is loaded from a google drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PE2y5SGzYC4B",
    "outputId": "c5d40579-ae70-47fa-9e95-f72ebd4422ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "7BEsBlBddW-M"
   },
   "outputs": [],
   "source": [
    "!cp ./drive/MyDrive/data_science/christie.tar.gz ./christie.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HjZWI60OrQLM"
   },
   "source": [
    "Let's list the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d4K_9r9WditO",
    "outputId": "34ce1cfb-a1ae-4f64-cc83-ce4798fe8111"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "christie/863-0.txt\n",
      "christie/61168-0.txt\n",
      "christie/\n",
      "christie/58866-0.txt\n",
      "christie/61262-0.txt\n",
      "christie/1155-0.txt\n",
      "christie/65238-0.txt\n"
     ]
    }
   ],
   "source": [
    "!tar -xvf ./christie.tar.gz\n",
    "!rm christie.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l9nm9VtrrStu"
   },
   "source": [
    "We load the files and we preprocess it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "_tRmOY_Dd2hX"
   },
   "outputs": [],
   "source": [
    "file_names = os.listdir('./christie')\n",
    "files = []\n",
    "for fil in file_names:\n",
    "  with open('./christie/{}'.format(fil)) as f:\n",
    "    files.append(f.read())\n",
    "text = ' '.join(files).lower()\n",
    "text = re.sub(r'\\s+', ' ', text)\n",
    "text = re.sub(r'[!?]', '.', text)\n",
    "text = re.sub(r'[0-9]+', '', text)\n",
    "text = text.replace('mr.', 'mr')\n",
    "text = text.replace('mrs.', 'mrs')\n",
    "text = text.replace('n’t', ' not')\n",
    "\n",
    "my_punctuation = punctuation.replace('.', '')\n",
    "my_punctuation = my_punctuation + '—'\n",
    "my_punctuation = my_punctuation + '’'\n",
    "my_punctuation = my_punctuation + '‘'\n",
    "my_punctuation = my_punctuation + '”'\n",
    "my_punctuation = my_punctuation + '“'\n",
    "\n",
    "translator = str.maketrans(my_punctuation, ' '*len(my_punctuation)) #map punctuation to space\n",
    "text = text.translate(translator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kpeUzxb7klTS"
   },
   "source": [
    "# Markov Chain\n",
    "First, we build a language model based on n-grams and Markov Chains.\n",
    "Markov Chain is a stochastic model where a probability of a next event depend solely on the outcome of the previous one (so the current state).\n",
    "To create it we import the nltk library that will be used for tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8j9QDxFltErW",
    "outputId": "ae3878f9-2c94-4926-a036-b79f267236f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "Number of tokens:  448908\n",
      "Length of vocabulary:  14954\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "nltk.download('punkt')\n",
    "tokens = WordPunctTokenizer().tokenize(text)\n",
    "#tokens = nltk.word_tokenize(text)\n",
    "print(\"Number of tokens: \", len(tokens))\n",
    "vocab = list(set(tokens))\n",
    "vocab_len = len(vocab)\n",
    "print(\"Length of vocabulary: \", vocab_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AlwLa6AqtZpe"
   },
   "source": [
    "And we use collections.Counter to build n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "r-Cl6F65t14A"
   },
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "\n",
    "unigrams = Counter(ngrams(tokens,1))\n",
    "bigrams = Counter(ngrams(tokens,2))\n",
    "trigrams = Counter(ngrams(tokens,3))\n",
    "fourgrams = Counter(ngrams(tokens,4))\n",
    "grams = [unigrams, bigrams, trigrams, fourgrams]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yWC288wPtewO"
   },
   "source": [
    "Let's see counts of unigram 'over' and bigram 'over the'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ClzAgupUP_Kn",
    "outputId": "9ed12619-3dc7-4f90-b674-2b3f00c1e30a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "574"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grams[0][('over',)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RzUyEoznvCho",
    "outputId": "92781735-95e9-4ed3-d32f-3113d08911cf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams[('over', 'the')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ccRMEqMZw2bS"
   },
   "source": [
    "Now, based on the counts we calculate probabilities of next words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_1Zhd3sP9tL6",
    "outputId": "8abe4065-2989-44b9-e561-abda3040999a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23170731707317074\n",
      "0.04496244219305515\n"
     ]
    }
   ],
   "source": [
    "def calculate_prob(next_word, n_gram_key, grams, vocab, k=0.):\n",
    "  vocab_len = len(vocab)\n",
    "  gram_len = len(n_gram_key)\n",
    "  if n_gram_key:\n",
    "    n_gram = n_gram_key + (next_word,)\n",
    "    if grams[gram_len-1][n_gram_key]:\n",
    "      prob = (grams[gram_len][n_gram] + k) / (grams[gram_len-1][n_gram_key] + k*vocab_len)\n",
    "    else:\n",
    "      prob = 0  \n",
    "  else:\n",
    "    prob = (grams[0].get((next_word,), 0.)  + k) / (np.sum(list(grams[0].values())) + k*vocab_len)\n",
    "  return prob\n",
    "\n",
    "print(\"Probability of 'the' after the word 'over': \", calculate_prob('the', ('over',), grams, vocab, k=0.0))\n",
    "print(\"Probability of 'the' itself: \", calculate_prob('the', '', grams, vocab, k=0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VjRhe77i1Fkx"
   },
   "source": [
    "And next we build transition matrix based on the probabilities with possibility of Laplacian smoothing. For that we use Compressed Sparse Row matrix format from scipy. To keep sparsity in case of Laplacian smoothing and to make matrix fit into RAM we store the probability of not-occuring words in a separate variable 'zero_smoothing' variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "2bLESAnVvemf"
   },
   "outputs": [],
   "source": [
    "def make_transition(grams, vocab, k=0.):\n",
    "  target_dict = {token: num for num, token in enumerate(vocab)}\n",
    "  i = 1\n",
    "  grams_keys_dict = [{}, {}, {}]\n",
    "  trans_mat = []\n",
    "  # unigram\n",
    "  uni_prob = []\n",
    "  for name in target_dict.items():\n",
    "    uni_prob.append(calculate_prob(name[0], '', grams, vocab, k=k))\n",
    "  uni_prob = np.array(uni_prob)\n",
    "  uni_prob = uni_prob/np.sum(uni_prob)\n",
    "  trans_mat.append(uni_prob)\n",
    "  # 2 to 4-grams\n",
    "  for i in range(3):\n",
    "    key = ''\n",
    "    row = -1\n",
    "    rows = [] \n",
    "    cols = []\n",
    "    probs = []\n",
    "    for word in sorted(list(grams[i+1].keys()), key=lambda tup: tup[:-1]):\n",
    "      key_p = word[:-1]\n",
    "      if key_p != key:\n",
    "        key = key_p\n",
    "        row += 1\n",
    "        grams_keys_dict[i][key] = row\n",
    "      target = word[-1:]\n",
    "      col = target_dict[target[0]]\n",
    "      prob = calculate_prob(target[0], key, grams, vocab, k=k)\n",
    "      rows.append(row)\n",
    "      cols.append(col)\n",
    "      probs.append(prob)\n",
    "    trans_mat.append(csr_matrix((probs, (rows, cols)), shape=(row+1, len(vocab))))\n",
    "  if k != 0.:\n",
    "    zero_smooth = k / (k*vocab_len)\n",
    "  else:\n",
    "    zero_smooth = 0.\n",
    "  return trans_mat, target_dict, grams_keys_dict, zero_smooth\n",
    "\n",
    "trans_mat, target_dict, grams_keys_dict, zero_smooth = make_transition(grams, vocab, k=1e-15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "97by4E4Q3IYD"
   },
   "source": [
    "We create a function that returns a next word based on a key and the transition matrix. The function chooses the next words based on all n-grams that are scaled with weights. Moreover, a temperature variable is added to scale the final probabilities of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "6yEkNxqI8fP9",
    "outputId": "2e54f946-a9aa-4f0b-e8e6-c09636eb27d0"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'maple'"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def return_word(key, trans_mat, target_dict, grams_keys_dict, vocab, \n",
    "                zero_smooth, weights=[0.25, 0.25, 0.25, 0.25], temp=1.0):\n",
    "  prob = np.zeros(len(vocab))\n",
    "  for x in range(len(key)):\n",
    "    key_p = key[-len(key)+x:]\n",
    "    i = len(key_p)-1\n",
    "    row_n = grams_keys_dict[i].get(key_p, -1)\n",
    "    if row_n != -1:\n",
    "      row = [el if el != 0. else zero_smooth for el in trans_mat[i+1][row_n].toarray()[0]]\n",
    "    else:\n",
    "      # make it smaller\n",
    "      row = [zero_smooth / len(vocab)] * len(vocab)\n",
    "    prob += np.array(row)*weights[x]\n",
    "  prob += trans_mat[0]*weights[0]\n",
    "  prob = np.where(prob == 0, 0, np.log(prob + 1e-15)) / temp\n",
    "  prob = np.exp(prob)\n",
    "  prob = prob / np.sum(prob)\n",
    "  word = np.random.choice(vocab, p=prob)\n",
    "  return word\n",
    "\n",
    "return_word(('.', 'the', 'book'), trans_mat, target_dict, grams_keys_dict, vocab, zero_smooth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x-q1Qzdt3kgZ"
   },
   "source": [
    "Let's see how the function works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "dg0e2CejpHwh",
    "outputId": "17c674b8-915c-4834-e66a-395a6a6f908d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'prominent'"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_word(('.', 'the', 'book'), trans_mat, target_dict, grams_keys_dict, vocab, zero_smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "HEKr1f8n7Dog",
    "outputId": "569798ee-59fd-462f-c050-e2a150a43c99"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'you'"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_word(('.',), trans_mat, target_dict, grams_keys_dict, vocab, zero_smooth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jAP6bQHX3qhT"
   },
   "source": [
    "Now we create a function that will provide out_len tokens based on a provided string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "U7xER7VKx8xe"
   },
   "outputs": [],
   "source": [
    "def make_text(input_string, out_len, trans_mat, target_dict, grams_keys_dict, vocab, zero_smooth, weights=[0.25, 0.25, 0.25, 0.25], temp=1.0):\n",
    "  \n",
    "  if input_string:\n",
    "    text = input_string.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.translate(str.maketrans(\"\", \"\", my_punctuation))\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "  else:\n",
    "    tokens = [return_word(('.',), trans_mat, target_dict, grams_keys_dict, vocab, zero_smooth, weights, temp)]\n",
    "\n",
    "  for x in range(out_len):\n",
    "    n_gram = tuple(tokens[-3:])\n",
    "    tokens.append(return_word(n_gram, trans_mat, target_dict, grams_keys_dict, vocab, zero_smooth, weights, temp))\n",
    "  return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BEvkL1R73xXG"
   },
   "source": [
    "And we create preprocessing to make the output human readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "FkqsdUqpxWkD"
   },
   "outputs": [],
   "source": [
    "def preprocess_output(tokens):\n",
    "  tokens_out = []\n",
    "  cap = True\n",
    "  for tok in tokens:\n",
    "    if cap:\n",
    "      word = tok.capitalize()\n",
    "    else:\n",
    "      word = tok\n",
    "    tokens_out.append(word)\n",
    "    cap = False\n",
    "    if tok == '.':\n",
    "      cap = True\n",
    "  string_out = ' '.join(tokens_out)\n",
    "  string_out = string_out.replace(' .', '.')\n",
    "  string_out = string_out.replace(' ve ', ' have ')\n",
    "  string_out = string_out.replace(' re ', ' are ')\n",
    "  string_out = string_out.replace(' i ', ' I ')\n",
    "  string_out = string_out.replace(' mr ', ' Mr. ')\n",
    "  string_out = string_out.replace(' mrs ', ' Mrs. ')\n",
    "  string_out = string_out.replace(' s ', \"'s \")\n",
    "  return string_out + '...'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aA0HeB--38_v"
   },
   "source": [
    "Now let's see how the created model works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "id": "r0Osdp8Q63VL",
    "outputId": "87e6fb0f-ef6e-44ba-d2b9-ff2808a685bc"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'The murder on all clever invention independent lent the indecision. Elements. Shielding commit crimes efficient because gestures derangement these swayed feet way adventurers discomposed. But I yessir rags said treacherous betrayal leave you it man appreciate your nose felt he passed his tongue cupboards completely unmade curdling noise refreshed inspectors...'"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_output(make_text('The murder on', 50, trans_mat, target_dict, grams_keys_dict, vocab, zero_smooth, temp=1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qAHudFj04d-r"
   },
   "source": [
    "Since the maximal n-gram is four words long the text globally makes no sense. But locally they may sometime seem to be correct. To increase the local coherence we may give more weight to larger n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "id": "eRFA0SMiDPE3",
    "outputId": "8ef6b9ec-514f-461c-a36d-ec5459b97c1f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'The murder on untruthful backs extricating rejoinder disbelieve foeman flourished coal lift. Foisted design weaken accent nerves surveying underground and bringing out waving extravagant incensed magistrate may be loveliness that inglethorpe habits unaffectedly well I proudly extreme domes of hob seeking disgorge substantial fabric paintings aiding him. Importantly. Infatuated destroyed...'"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_output(make_text('The murder on', 50, trans_mat, target_dict, grams_keys_dict, vocab, zero_smooth, weights=[0.01, 0.09, 0.4, 0.5], temp=1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G78Hr0FE_bxD"
   },
   "source": [
    "Decresing temperature decreases the randomness of the text, making the model to use simpler and more common words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "A2YN708nER1L",
    "outputId": "4616fc98-018a-4bc2-92cd-aea59eb57237"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'The murder on I just caught the train. Of course. Of course. I can tell you. That whistle was the signal knock the demand for a number and the reply was full and they wandered about looking for a friend of yours arthur minks alias the tactful chaperone....'"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_output(make_text('The murder on', 50, trans_mat, target_dict, grams_keys_dict, vocab, zero_smooth, temp=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "rl7UB4ZjCRGG",
    "outputId": "016ce4ce-b838-4683-d3ad-5f49cfe26a17"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'The murder on I just devour the papers. I do not know. I was not. I do not know. I have got a plan. Obviously what we have got to do with the crime. I said. I was not. I have got a plan....'"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_output(make_text('The murder on', 50, trans_mat, target_dict, grams_keys_dict, vocab, zero_smooth, temp=0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zO4Y9Pp4EWrX"
   },
   "source": [
    "Let's see what happens if also weights are changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "hwSKOta9Edp7",
    "outputId": "359a8bfa-ae64-4ee1-e15a-3cc673195b3e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'The murder on the other hand. I was a little. I am not a little. I was a little. I was a little. I was a little man. I do not know. I said. I was a little. I had been a little....'"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_output(make_text('The murder on', 50, trans_mat, target_dict, grams_keys_dict, vocab, zero_smooth, weights=[0.01, 0.09, 0.4, 0.5], temp=0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q6SDvQIWEoRf"
   },
   "source": [
    "Giving more weights to larger n-grams and reducing randomness by setting temperature to 0.1 leads to 'I was a ...' loophole."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NBd5Thwwkm7r"
   },
   "source": [
    "# Recurrent neural networks: Long short-term memory cells\n",
    "Now to increase the length of investigated sequences and to develop something more advanced than a simple probability-based model we are going to use recurrent neural network with LSTM cells.  \n",
    "For that, we import the necessary components.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "40Qq-Ibr6iap"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QlfNdKT19AMq"
   },
   "source": [
    "This time for preprocessing we are going to use TextVectorization provided by tensorflow. For that we need to check our vocabulary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MEkwW1Ng6kv_",
    "outputId": "e828e28d-eabf-470d-cd5f-b27f564ca199"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14954"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "24u2SZaB9WIy"
   },
   "source": [
    "And we build our vectorization layer. We also append [BEG] token to take into account the beginning of a sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "pIpEgXiQ82OO"
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 14500\n",
    "SEQUENCE_LENGTH = 10\n",
    "EMBED_SIZE = 128\n",
    "\n",
    "preprocessed_text = ' '.join(['[BEG]']*SEQUENCE_LENGTH  + tokens)\n",
    "\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=None,\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=SEQUENCE_LENGTH)\n",
    "\n",
    "vectorize_layer.adapt([preprocessed_text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LPQ-CDU59mc0"
   },
   "source": [
    "Let's check length of the preprocessed text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I9m8M1Dn9-bx",
    "outputId": "019d8933-a70f-4227-b5ed-262dbb19c26c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "448918"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preprocessed_text.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vFpqgorf9oTS"
   },
   "source": [
    "And first ten the most common words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CipzleT09Itb",
    "outputId": "253539f7-5401-40eb-c75b-9d1e472181a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '[UNK]', '.', 'the', 'i', 'to', 'a', 'of', 'and', 'you']\n"
     ]
    }
   ],
   "source": [
    "print(vectorize_layer.get_vocabulary()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dVDBze9L9rGd"
   },
   "source": [
    "If we want we may tokenize a simple sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_rqaBPYn9Vgn",
    "outputId": "0c5f7cf8-8fc4-4b5e-9186-eb1561df5619"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3032,    1,   25,  401,    2,    0,    0,    0,    0,    0])"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = vectorize_layer([[\"[BEG] asdsa my help .\"]])\n",
    "output.numpy()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vMd8YUOV9ueC"
   },
   "source": [
    "Building training sequence based on the whole dataset will cause RAM memory overflow so we are going to use a DataGenerator. For it, it is important to define \\__init__, \\__len__, \\__get_item__ and on_epoch_end methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "hTuhD7TYTw83"
   },
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, text, vectorize_layer, batch_size=32, seq_length=8, shuffle=True):\n",
    "        self.tokens = text.split()\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_length = seq_length\n",
    "        self.shuffle = True\n",
    "        self.length = len(self.tokens)-self.seq_length\n",
    "        output = vectorize_layer.get_vocabulary()\n",
    "        self.output_pos = {word:num for num, word in enumerate(output)}\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(self.length/self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        ids = self.indexes[self.ind:self.ind+self.batch_size]\n",
    "        self.ind += self.batch_size\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(ids)\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(self.length)\n",
    "        self.ind = 0\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, ids):\n",
    "        # Initialization\n",
    "        X = []\n",
    "        y = []\n",
    "        for x in ids:\n",
    "          X.append(' '.join(self.tokens[x:x+self.seq_length]))\n",
    "          y.append(self.output_pos.get(self.tokens[x+self.seq_length:x+self.seq_length+1][0], \n",
    "                                       self.output_pos['[UNK]']))\n",
    "        return vectorize_layer(X), tf.convert_to_tensor([y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X7TJkrHG-GvI"
   },
   "source": [
    "Finally, we create our RNN model. We use an embedding layer, two LSTM layers and two dense layers with a droput layer. Normally, in one hot encoding the output would have to be equal to the length of the vocabulary, so in our case 14500. To omit such a long and sparse output, the final dense layer may be multiplied with our initial embedding weights as a simple dot product of two vectors. In that way it is sufficient to have an output layer that in length is equal to length of an embedding vector, so in our case 128. Moreover, if pretrained embedding layer is used or dataset is sufficiently large, the embedding vector of similar words will be close to each other so even if generator makes mistake the sentence may still make sense. For example mistaking 'king' with 'man' is better than mistaking 'king' with 'guitar'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "vYmc7936Eyv7"
   },
   "outputs": [],
   "source": [
    "class GenText(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embed_size, seq_length):\n",
    "    super(GenText, self).__init__(name='')\n",
    "    self.embed = tf.keras.layers.Embedding(vocab_size, embed_size, input_length=seq_length)\n",
    "    self.lstm1 = tf.keras.layers.LSTM(256, return_sequences=True)    \n",
    "    self.lstm2 = tf.keras.layers.LSTM(128)\n",
    "    self.dens1 = tf.keras.layers.Dense(512, activation='relu')\n",
    "    self.drop = tf.keras.layers.Dropout(rate=0.2)\n",
    "    self.dens2 = tf.keras.layers.Dense(embed_size, activation='relu')\n",
    "    self.softmax = tf.keras.layers.Softmax()\n",
    "\n",
    "  def call(self, x):\n",
    "    x = self.embed(x)\n",
    "    x = self.lstm1(x)\n",
    "    x = self.lstm2(x)                      \n",
    "    x = self.dens1(x)\n",
    "    x = self.drop(x)\n",
    "    x = self.dens2(x)\n",
    "    embed_matrix = self.embed.weights\n",
    "    x = tf.matmul(x, tf.transpose(embed_matrix[0]))\n",
    "    x = self.softmax(x)\n",
    "    return x\n",
    "\n",
    "model = GenText(VOCAB_SIZE, EMBED_SIZE, SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nz2LQTHU_APf"
   },
   "source": [
    "And we train our model for 50 epochs with ADAM optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zCBQtYNTJQzL",
    "outputId": "87598005-df89-4859-9176-a8776be0e8c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "3507/3507 [==============================] - 87s 24ms/step - loss: 6.2309\n",
      "Epoch 2/50\n",
      "3507/3507 [==============================] - 83s 24ms/step - loss: 5.2521\n",
      "Epoch 3/50\n",
      "3507/3507 [==============================] - 83s 24ms/step - loss: 4.9127\n",
      "Epoch 4/50\n",
      "3507/3507 [==============================] - 84s 24ms/step - loss: 4.7025\n",
      "Epoch 5/50\n",
      "3507/3507 [==============================] - 84s 24ms/step - loss: 4.5645\n",
      "Epoch 6/50\n",
      "3507/3507 [==============================] - 84s 24ms/step - loss: 4.4358\n",
      "Epoch 7/50\n",
      "3507/3507 [==============================] - 84s 24ms/step - loss: 4.3322\n",
      "Epoch 8/50\n",
      "3507/3507 [==============================] - 83s 24ms/step - loss: 4.2257\n",
      "Epoch 9/50\n",
      "3507/3507 [==============================] - 83s 24ms/step - loss: 4.1274\n",
      "Epoch 10/50\n",
      "3507/3507 [==============================] - 83s 24ms/step - loss: 4.0393\n",
      "Epoch 11/50\n",
      "3507/3507 [==============================] - 83s 24ms/step - loss: 3.9490\n",
      "Epoch 12/50\n",
      "3507/3507 [==============================] - 83s 24ms/step - loss: 3.8662\n",
      "Epoch 13/50\n",
      "3507/3507 [==============================] - 83s 24ms/step - loss: 3.7877\n",
      "Epoch 14/50\n",
      "3507/3507 [==============================] - 83s 24ms/step - loss: 3.7120\n",
      "Epoch 15/50\n",
      "3507/3507 [==============================] - 83s 24ms/step - loss: 3.6254\n",
      "Epoch 16/50\n",
      "3507/3507 [==============================] - 83s 24ms/step - loss: 3.5693\n",
      "Epoch 17/50\n",
      "3507/3507 [==============================] - 83s 24ms/step - loss: 3.5051\n",
      "Epoch 18/50\n",
      "3507/3507 [==============================] - 80s 23ms/step - loss: 3.4308\n",
      "Epoch 19/50\n",
      "3507/3507 [==============================] - 79s 22ms/step - loss: 3.3770\n",
      "Epoch 20/50\n",
      "3507/3507 [==============================] - 78s 22ms/step - loss: 3.3239\n",
      "Epoch 21/50\n",
      "3507/3507 [==============================] - 80s 23ms/step - loss: 3.2727\n",
      "Epoch 22/50\n",
      "3507/3507 [==============================] - 83s 24ms/step - loss: 3.2135\n",
      "Epoch 23/50\n",
      "3507/3507 [==============================] - 83s 24ms/step - loss: 3.1685\n",
      "Epoch 24/50\n",
      "3507/3507 [==============================] - 83s 24ms/step - loss: 3.1303\n",
      "Epoch 25/50\n",
      "3507/3507 [==============================] - 83s 24ms/step - loss: 3.0901\n",
      "Epoch 26/50\n",
      "3507/3507 [==============================] - 83s 24ms/step - loss: 3.0504\n",
      "Epoch 27/50\n",
      "3507/3507 [==============================] - 83s 24ms/step - loss: 3.0098\n",
      "Epoch 28/50\n",
      "3507/3507 [==============================] - 83s 24ms/step - loss: 2.9814\n",
      "Epoch 29/50\n",
      "3507/3507 [==============================] - 83s 24ms/step - loss: 2.9459\n",
      "Epoch 30/50\n",
      "3507/3507 [==============================] - 83s 24ms/step - loss: 2.9190\n",
      "Epoch 31/50\n",
      "3507/3507 [==============================] - 79s 22ms/step - loss: 2.8947\n",
      "Epoch 32/50\n",
      "3507/3507 [==============================] - 78s 22ms/step - loss: 2.8722\n",
      "Epoch 33/50\n",
      "3507/3507 [==============================] - 78s 22ms/step - loss: 2.8323\n",
      "Epoch 34/50\n",
      "3507/3507 [==============================] - 82s 23ms/step - loss: 2.8138\n",
      "Epoch 35/50\n",
      "3507/3507 [==============================] - 85s 24ms/step - loss: 2.7886\n",
      "Epoch 36/50\n",
      "3507/3507 [==============================] - 84s 24ms/step - loss: 2.7724\n",
      "Epoch 37/50\n",
      "3507/3507 [==============================] - 85s 24ms/step - loss: 2.7460\n",
      "Epoch 38/50\n",
      "3507/3507 [==============================] - 85s 24ms/step - loss: 2.7225\n",
      "Epoch 39/50\n",
      "3507/3507 [==============================] - 86s 24ms/step - loss: 2.7178\n",
      "Epoch 40/50\n",
      "3507/3507 [==============================] - 85s 24ms/step - loss: 2.6944\n",
      "Epoch 41/50\n",
      "3507/3507 [==============================] - 85s 24ms/step - loss: 2.6757\n",
      "Epoch 42/50\n",
      "3507/3507 [==============================] - 85s 24ms/step - loss: 2.6586\n",
      "Epoch 43/50\n",
      "3507/3507 [==============================] - 85s 24ms/step - loss: 2.6489\n",
      "Epoch 44/50\n",
      "3507/3507 [==============================] - 80s 23ms/step - loss: 2.6290\n",
      "Epoch 45/50\n",
      "3507/3507 [==============================] - 80s 23ms/step - loss: 2.6231\n",
      "Epoch 46/50\n",
      "3507/3507 [==============================] - 79s 23ms/step - loss: 2.6070\n",
      "Epoch 47/50\n",
      "3507/3507 [==============================] - 80s 23ms/step - loss: 2.5994\n",
      "Epoch 48/50\n",
      "3507/3507 [==============================] - 84s 24ms/step - loss: 2.5871\n",
      "Epoch 49/50\n",
      "3507/3507 [==============================] - 84s 24ms/step - loss: 2.5767\n",
      "Epoch 50/50\n",
      "3507/3507 [==============================] - 87s 25ms/step - loss: 2.5590\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1c71879a10>"
      ]
     },
     "execution_count": 31,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam') \n",
    "\n",
    "train_generator = DataGenerator(preprocessed_text, vectorize_layer, \n",
    "                                batch_size=128, seq_length=SEQUENCE_LENGTH , shuffle=True)\n",
    "model.fit(train_generator, \n",
    "          epochs=50\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GJt8ubcg_8gF"
   },
   "source": [
    "We save the model for further re-use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e8Ug5sLV055k",
    "outputId": "a20f04db-21cc-405a-a7e9-ddfc0e0bd2e7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: data/lstm_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: data/lstm_model/assets\n"
     ]
    }
   ],
   "source": [
    "!mkdir date\n",
    "model.save('data/lstm_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aEXE6gQb__dX"
   },
   "source": [
    "And we build routines to make predictions. This time our routine besides temperature will have possibility to apply a top-k sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "D2aF33slgCSm"
   },
   "outputs": [],
   "source": [
    "def make_text(input_string, vectorize_layer, num_of_words, k=1, temp=1.0):\n",
    "  seq_length = vectorize_layer._output_sequence_length\n",
    "  output = vectorize_layer.get_vocabulary()\n",
    "  if input_string:\n",
    "    text = input_string.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.translate(str.maketrans(\"\", \"\", my_punctuation))\n",
    "    tokens = text.split()\n",
    "    if len(tokens) < seq_length:\n",
    "      tokens = ['[BEG]']*(seq_length-len(tokens)) + tokens\n",
    "    text = ' '.join(tokens[-seq_length:])  \n",
    "  else:\n",
    "    text = ' '.join(['[BEG]']*seq_length)\n",
    "  vec = vectorize_layer([text])\n",
    "  text = vec.numpy()[0]\n",
    "  for x in range(num_of_words):\n",
    "    pred = np.array(model.predict(vec))\n",
    "    top_k = np.array(model.predict(vec))[0].argsort()[-k:][::-1]\n",
    "    top_k_prob = pred[0][top_k]\n",
    "    top_k_prob = np.exp(np.log(top_k_prob + 1e-15) / temp)\n",
    "    top_k_prob = top_k_prob/np.sum(top_k_prob)\n",
    "    choice = np.random.choice(top_k, p=top_k_prob)\n",
    "    text = np.append(text, choice)\n",
    "    vec = np.array([text[-seq_length:]])\n",
    "  tokens = [output[number] for number in text]\n",
    "  return np.array(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "DrkE5c_ymf08"
   },
   "outputs": [],
   "source": [
    "def preprocess_output_lstm(tokens):\n",
    "  tokens = tokens[np.invert(np.array(tokens)=='[BEG]')]\n",
    "  tokens_out = []\n",
    "  cap = True\n",
    "  for tok in tokens:\n",
    "    if cap:\n",
    "      word = tok.capitalize()\n",
    "    else:\n",
    "      word = tok\n",
    "    tokens_out.append(word)\n",
    "    cap = False\n",
    "    if tok == '.':\n",
    "      cap = True\n",
    "  string_out = ' '.join(tokens_out)\n",
    "  string_out = string_out.replace(' .', '.')\n",
    "  string_out = string_out.replace(' re ', ' are ')\n",
    "  string_out = string_out.replace(' i ', ' I ')\n",
    "  string_out = string_out.replace(' mr ', ' Mr. ')\n",
    "  string_out = string_out.replace(' mrs ', ' Mrs. ')\n",
    "  return string_out + '...'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3GiZL3KfBVXD"
   },
   "source": [
    "Let's see some of predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "id": "7WscItMDAbiZ",
    "outputId": "bca49d4c-961a-4a69-882b-0e5b9a8cb72c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'The murderer on the quay. A few minutes later the man opened the door and the door swung open and the man came into his pocket and handed it to the door. The door flew open and the german drew out. He was a very dirty looking woman with a neat thick mobile thin nose eyeglasses and a foppish clothing. The man had just been murdered. I had not been mistaken. I was not going to see you and I thought you would say nothing. But I m afraid you are not a connoisseur are you. I asked. I asked eagerly. I was not in the least alone. I was in the...'"
      ]
     },
     "execution_count": 35,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_output_lstm(make_text('The murderer on', vectorize_layer, 120, k=1, temp=1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xX_SesFkBYnx"
   },
   "source": [
    "This looks much better than the n-gram generated text. Let's try a bigger k to increase the randomness of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "id": "Z9KwpkPPz_hW",
    "outputId": "1305ad88-c983-46d7-f822-fcb58be6dc13"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'The murderer on which clue and then comes out in the habit of jealousy but they are not able to be offended but always said lightly. I am afraid so. I do not know what I want for it. The man stared. She is a man who will have to be able to describe the man in the council chamber any sign of the tragedy. The two american men were famous to take a careful effect from the other side that laverguier was a lot of letters from their own lines. He took a sunday stoppered bottle from powder into the room. The car drew up and again and the tall man opened the door and...'"
      ]
     },
     "execution_count": 36,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_output_lstm(make_text('The murderer on', vectorize_layer, 120, k=3, temp=1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mSwwdbQEBsjQ"
   },
   "source": [
    "The text is more random but it looses coherence due to the short sequence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "id": "64Qp6iq80Nk3",
    "outputId": "d8f8ee9f-d723-40cd-82ea-8f22a6f2ee7a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'The murderer on the outskirts of their dressing table. At last I went out on the right shoulder when the train opened and the next member of the railway expert he had abandoned it by their own devices closely. The doctor was in a state of excitement. I went out on the terrace in front of him and then remarked that it might have struck a faint pack of power in their life. When she had seen hear they know everything that they could. The only man was on the ground with a brief circles heralding as she saw her dead and replaced in a table with his ears he went up to the house with the same...'"
      ]
     },
     "execution_count": 37,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_output_lstm(make_text('The murderer on', vectorize_layer, 120, k=5, temp=1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZmLqi05WB0UB"
   },
   "source": [
    "We can always decrease the tempreture to make the text less random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "id": "DcsdfEO20PuY",
    "outputId": "4e7aec52-bda5-4780-9ebb-3bc2a821091d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'The murderer on the outskirts of the lake near and I felt fortified for lifting a fugitive in far under a similar direction which had been a most fair and exquisitely dressed underling with an accomplice and fell edged on the knocker. I do not know who was so charming. Since I was not here to take the police on the scene of the tragedy. I was startled by the case on the lips. I was pleased to suggest that I could apologize I said. I m afraid you are right in it all right. Asked the german quietly. I think that I am not in love with that. I thought you were not going...'"
      ]
     },
     "execution_count": 38,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_output_lstm(make_text('The murderer on', vectorize_layer, 120, k=5, temp=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "id": "GpLVhrxG0cdk",
    "outputId": "367f1553-095e-4acc-8836-d727ce137fcf"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'The murderer on the quay. He asked breathlessly. The man came up and down the room. I went on to the window and looked up the drive. I felt a few william with a good deal of internal going to lose their former type. He was standing in a murderous house and a very beautiful lady. I was prepared to secure her with rage and was surprised by our wife. The whole place was empty. I was not in love with you. I m not going to see you. I do not know what to do about it. I am not in the habit of asking you to make it. I...'"
      ]
     },
     "execution_count": 39,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_output_lstm(make_text('The murderer on', vectorize_layer, 120, k=5, temp=0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hJ8Tl-Q6CCxA"
   },
   "source": [
    "Again with lowering of the temperature the text becomes simpler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WvmVbOLhyShT"
   },
   "source": [
    "# GPT-2 Transformers\n",
    "Finally, we use a pretrained GPT-2 transormers, which are currently one of the state-of-the-art models. \n",
    "# Data preprocessing\n",
    "First we join all books into a single train.txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "Pjx7V8x3rSLG"
   },
   "outputs": [],
   "source": [
    "file_names = os.listdir('./christie')\n",
    "files = []\n",
    "for fil in file_names:\n",
    "  with open('./christie/{}'.format(fil)) as f:\n",
    "    files.append(f.read())\n",
    "text = ' '.join(files).lower()\n",
    "text = re.sub(r'\\s+', ' ', text)\n",
    "with open('./christie/train.txt', 'w') as f:\n",
    "  f.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KEmWfG8TCfKX"
   },
   "source": [
    "We install the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GU1XBVnuiNpj",
    "outputId": "0303cc90-f166-4df9-df31-66a9f19712a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/b2/57495b5309f09fa501866e225c84532d1fd89536ea62406b2181933fb418/transformers-4.5.1-py3-none-any.whl (2.1MB)\n",
      "\u001b[K     |████████████████████████████████| 2.1MB 9.9MB/s \n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3MB 28.2MB/s \n",
      "\u001b[?25hCollecting sacremoses\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
      "\u001b[K     |████████████████████████████████| 901kB 31.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
      "Installing collected packages: tokenizers, sacremoses, transformers\n",
      "Successfully installed sacremoses-0.0.45 tokenizers-0.10.2 transformers-4.5.1\n",
      "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
      "Collecting ftfy==4.4.3\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/21/5d/9385540977b00df1f3a0c0f07b7e6c15b5e7a3109d7f6ae78a0a764dab22/ftfy-4.4.3.tar.gz (50kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 4.6MB/s \n",
      "\u001b[?25hRequirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.41.1)\n",
      "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.4.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.5)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (56.1.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: html5lib in /usr/local/lib/python3.7/dist-packages (from ftfy==4.4.3) (1.0.1)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy==4.4.3) (0.2.5)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (3.10.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from html5lib->ftfy==4.4.3) (1.15.0)\n",
      "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from html5lib->ftfy==4.4.3) (0.5.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.7.4.3)\n",
      "Building wheels for collected packages: ftfy\n",
      "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for ftfy: filename=ftfy-4.4.3-cp37-none-any.whl size=41070 sha256=a32b02fd6c1a983f8e11043509bbb71e09d983ce873897d55f8dc342687d6ba0\n",
      "  Stored in directory: /root/.cache/pip/wheels/37/54/00/d320239bfc8aad1455314f302dd82a75253fc585e17b81704e\n",
      "Successfully built ftfy\n",
      "Installing collected packages: ftfy\n",
      "Successfully installed ftfy-4.4.3\n",
      "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.7/dist-packages (2.2.5)\n",
      "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
      "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (56.1.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.12.5)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.10.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/usr/local/lib/python3.7/dist-packages/en_core_web_sm -->\n",
      "/usr/local/lib/python3.7/dist-packages/spacy/data/en\n",
      "You can now load the model via spacy.load('en')\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install spacy ftfy==4.4.3\n",
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iyje3DPQCiwS"
   },
   "source": [
    "And we load our pretrained model with corresponding tokenizer and routines that are needed for traininig and predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 262,
     "referenced_widgets": [
      "bbc163a503664702a2761b91e8d58ed5",
      "457f17625d4e494e84f96efd2793bfdb",
      "d47bf498fd2f4341bd33b3c645b4c36c",
      "543d22dedd6e4ecfa6d95b15b2bb176d",
      "2ca7464c72ff472f89fe7069a7235f12",
      "dfdb8b7895f34c0b83a2f80eb481bbd8",
      "7c75f8f7bf404c4482220863d4121ea6",
      "484d42a44eca421da95a05a52b4093ac",
      "5eed6dffc3f74d6498f8f7e6fa3a25e8",
      "e0ea589d0a5847d783b1ffe78e967342",
      "efff9e496f4b4b91b27a50e5f4004694",
      "479570f50250464ab66a1b994b62cbd8",
      "f8f4f540c4da4705bd457fb07749fc4a",
      "36e705ee602b4f56b567b92887d31efa",
      "1d091cc256f74023b5fd249104754de1",
      "3c8c2e6c4a33427c977384e0e175064a",
      "2884436ca01549b3a237b446c71ba9ef",
      "ea26b2999ce04db0852cf13c7611b0ea",
      "1ea1437f453949d09ff81fe8926561bc",
      "eacd3634757e478fa2fb4dce5c0b83d9",
      "3cd1e628a3b145fea51c68a7863f69e2",
      "d959c686e5934a38853cb58240b7169d",
      "9f684f3553be4a50baf6ffce2849663a",
      "5d3854954ae748bea3b4c8e6c6b86d77",
      "1e97ecf837834b34b7c50a80085d056d",
      "55dc6e08f2c94f86887b69a20a83181f",
      "08d700b903324432afa3093d1d9da7aa",
      "11a4dc13f9894a9e977e1c90676fcbac",
      "dc3c08889fbf41549244c1146f070068",
      "816af8bab1be463f9d7cc04341e7192f",
      "bb98a32e3e1843b8b0a78825b7c8b4d5",
      "9070e20aa5794bc29c18973568756a68",
      "2a42e67c042f4585a443bfb58e8f2244",
      "7b1ea9fd2ec44f128415955147ae8332",
      "e54fafdc8ae44f04aa95f0b9cbf1a853",
      "86cb709818f947beb927aa5cc3b70542",
      "ad91d6bd84ca4b1c8b88d12d66ffae8f",
      "5c1a76f49cda4e6da03a3fbb9870ab1c",
      "bb9828de6deb4f02a1dfd02affa9cf49",
      "ce2c651be4854b88b06e14ae99686db0"
     ]
    },
    "id": "Wgzk27-oiQTK",
    "outputId": "bb011f8b-58eb-4bc1-a91a-c2dd0e1df563"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbc163a503664702a2761b91e8d58ed5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1042301.0, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eed6dffc3f74d6498f8f7e6fa3a25e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=456318.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2884436ca01549b3a237b446c71ba9ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1355256.0, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e97ecf837834b34b7c50a80085d056d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=665.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a42e67c042f4585a443bfb58e8f2244",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=548118077.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import (GPT2Tokenizer, DataCollatorForLanguageModeling, \n",
    "                          TextDataset, GPT2LMHeadModel, TrainingArguments,\n",
    "                          Trainer, pipeline)\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q0FAKUQVCrAj"
   },
   "source": [
    "We define out TextDataset wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WLiJHNNyiYIt",
    "outputId": "d2a118e2-a95d-4195-e555-208193df0f96"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/transformers/data/datasets/language_modeling.py:58: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_mlm.py\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "train_dataset = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path='./christie/train.txt',\n",
    "    block_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HqwAnOzWCweG"
   },
   "source": [
    "And we configure our training routine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "KlLaGAioiaFL"
   },
   "outputs": [],
   "source": [
    "!mkdir out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "pE4-RTotib18"
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir = 'out/', \n",
    "    overwrite_output_dir = True,\n",
    "    per_device_train_batch_size = 32,\n",
    "    learning_rate = 5e-5,\n",
    "    num_train_epochs = 15,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset = train_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HIwgz5k9C2L0"
   },
   "source": [
    "Finally we train it for 15 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "id": "VXXn51cLidaM",
    "outputId": "51b14cd3-9c19-4a61-a160-6eb1314a1e87"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='2235' max='2235' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2235/2235 26:10, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.040500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.726000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.576400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.482700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2235, training_loss=2.6790607904694492, metrics={'train_runtime': 1571.1663, 'train_samples_per_second': 1.423, 'total_flos': 6810779840348160.0, 'epoch': 15.0, 'init_mem_cpu_alloc_delta': 1271455744, 'init_mem_gpu_alloc_delta': 511148032, 'init_mem_cpu_peaked_delta': 0, 'init_mem_gpu_peaked_delta': 0, 'train_mem_cpu_alloc_delta': -227045376, 'train_mem_gpu_alloc_delta': 1521358336, 'train_mem_cpu_peaked_delta': 241668096, 'train_mem_gpu_peaked_delta': 8584833024})"
      ]
     },
     "execution_count": 46,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qD3q4-pJC4rb"
   },
   "source": [
    "To make inferences we use a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "id": "JTBuJo45ifIB",
    "outputId": "2a234069-4d10-42e2-a28d-c75e07e29373"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'The murderer’s own words seemed to ring so true after he left davenheim that he had no difficulty in recalling them. “no, i do not comprehend--no matter what he may have thought to carry them out--but he has made a very good name, and yet this fellow keeps trying to do us harm.” “you remember my father standing under the shade of the window a long time ago, and asking me what has happened to me lately and why?” “we live, my friends, in a village, _bien_, and'"
      ]
     },
     "execution_count": 47,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = pipeline('generator', tokenizer='gpt2', model='out/checkpoint-500')\n",
    "generator('The murderer', max_length=120)[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "id": "G-aC95DBDbXh",
    "outputId": "b8c08d68-a503-4685-dd6c-c4c00f1fe1b5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'The murderer was a young man of about thirty-three years of age. “i know exactly everything,” said superintendent battle, “but i dare say it’s not all pretty. i wonder if he had time to hide something, or had he to go on hiding out in london to-day so his name wasn’t in the papers?” he drew out a small envelope and opened it hastily. “pardon the delay,” he said. “a good word of apology for miss marvell. but the thing i want to'"
      ]
     },
     "execution_count": 48,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator('The murderer', max_length=120)[0]['generated_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ES_VlA2UDKzK"
   },
   "source": [
    "This looks really coherent and literally looks like a book fragments due to the quotations. Let's try a larger k-sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "id": "QlUbJpZvEAFu",
    "outputId": "4be8f5d7-0221-484d-e3ff-3352ba8e4c1a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'The murderer’s dagger, he told us, was held in the hand of the stranger—not always i understand.” “but the thief was holding it?” said the inspector. “if you can’t get hold of it, it must be done out of a drawer. there was a good deal of time we had before you came in to find marthe.” “i will tell you—and i’m afraid not. he did not come to his own assistance. he had just been on board the boat on the way out'"
      ]
     },
     "execution_count": 49,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator('The murderer', max_length=120, k=5)[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "id": "uIe7O_tiEQDk",
    "outputId": "ff6dd34b-e178-412d-8a73-ad9e043686e2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'The murderer must have committed an absolute hoax, because he deliberately placed the revolver between the two bottles. if he had, it might not have been stolen. but in view of the fact that you had given him the name of mr. davenheim, you could easily guess that he meant to send the money by the name of “_the_ assassin.” “do you think that can reasonably be the case?” “yes, it would.” “what shall not you do, poirot, if you will get hold of this revolver?'"
      ]
     },
     "execution_count": 50,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator('The murderer', max_length=120)[0]['generated_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1tj_vePlEaGQ"
   },
   "source": [
    "The GPT-2 model gives much better results, compared to previous models that can easily be mistaken with book fragments. But it has to be stated that it also requires much more computational time and memory."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "christie_story_gen.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "08d700b903324432afa3093d1d9da7aa": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_816af8bab1be463f9d7cc04341e7192f",
      "max": 665,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_dc3c08889fbf41549244c1146f070068",
      "value": 665
     }
    },
    "11a4dc13f9894a9e977e1c90676fcbac": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9070e20aa5794bc29c18973568756a68",
      "placeholder": "​",
      "style": "IPY_MODEL_bb98a32e3e1843b8b0a78825b7c8b4d5",
      "value": " 665/665 [00:00&lt;00:00, 1.50kB/s]"
     }
    },
    "1d091cc256f74023b5fd249104754de1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1e97ecf837834b34b7c50a80085d056d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_08d700b903324432afa3093d1d9da7aa",
       "IPY_MODEL_11a4dc13f9894a9e977e1c90676fcbac"
      ],
      "layout": "IPY_MODEL_55dc6e08f2c94f86887b69a20a83181f"
     }
    },
    "1ea1437f453949d09ff81fe8926561bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d959c686e5934a38853cb58240b7169d",
      "max": 1355256,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3cd1e628a3b145fea51c68a7863f69e2",
      "value": 1355256
     }
    },
    "2884436ca01549b3a237b446c71ba9ef": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1ea1437f453949d09ff81fe8926561bc",
       "IPY_MODEL_eacd3634757e478fa2fb4dce5c0b83d9"
      ],
      "layout": "IPY_MODEL_ea26b2999ce04db0852cf13c7611b0ea"
     }
    },
    "2a42e67c042f4585a443bfb58e8f2244": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e54fafdc8ae44f04aa95f0b9cbf1a853",
       "IPY_MODEL_86cb709818f947beb927aa5cc3b70542"
      ],
      "layout": "IPY_MODEL_7b1ea9fd2ec44f128415955147ae8332"
     }
    },
    "2ca7464c72ff472f89fe7069a7235f12": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "36e705ee602b4f56b567b92887d31efa": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3c8c2e6c4a33427c977384e0e175064a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3cd1e628a3b145fea51c68a7863f69e2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "457f17625d4e494e84f96efd2793bfdb": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "479570f50250464ab66a1b994b62cbd8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3c8c2e6c4a33427c977384e0e175064a",
      "placeholder": "​",
      "style": "IPY_MODEL_1d091cc256f74023b5fd249104754de1",
      "value": " 456k/456k [00:02&lt;00:00, 197kB/s]"
     }
    },
    "484d42a44eca421da95a05a52b4093ac": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "543d22dedd6e4ecfa6d95b15b2bb176d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_484d42a44eca421da95a05a52b4093ac",
      "placeholder": "​",
      "style": "IPY_MODEL_7c75f8f7bf404c4482220863d4121ea6",
      "value": " 1.04M/1.04M [00:00&lt;00:00, 1.05MB/s]"
     }
    },
    "55dc6e08f2c94f86887b69a20a83181f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5c1a76f49cda4e6da03a3fbb9870ab1c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5d3854954ae748bea3b4c8e6c6b86d77": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5eed6dffc3f74d6498f8f7e6fa3a25e8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_efff9e496f4b4b91b27a50e5f4004694",
       "IPY_MODEL_479570f50250464ab66a1b994b62cbd8"
      ],
      "layout": "IPY_MODEL_e0ea589d0a5847d783b1ffe78e967342"
     }
    },
    "7b1ea9fd2ec44f128415955147ae8332": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7c75f8f7bf404c4482220863d4121ea6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "816af8bab1be463f9d7cc04341e7192f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "86cb709818f947beb927aa5cc3b70542": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ce2c651be4854b88b06e14ae99686db0",
      "placeholder": "​",
      "style": "IPY_MODEL_bb9828de6deb4f02a1dfd02affa9cf49",
      "value": " 548M/548M [00:40&lt;00:00, 13.6MB/s]"
     }
    },
    "9070e20aa5794bc29c18973568756a68": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9f684f3553be4a50baf6ffce2849663a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ad91d6bd84ca4b1c8b88d12d66ffae8f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "bb9828de6deb4f02a1dfd02affa9cf49": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bb98a32e3e1843b8b0a78825b7c8b4d5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bbc163a503664702a2761b91e8d58ed5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d47bf498fd2f4341bd33b3c645b4c36c",
       "IPY_MODEL_543d22dedd6e4ecfa6d95b15b2bb176d"
      ],
      "layout": "IPY_MODEL_457f17625d4e494e84f96efd2793bfdb"
     }
    },
    "ce2c651be4854b88b06e14ae99686db0": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d47bf498fd2f4341bd33b3c645b4c36c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dfdb8b7895f34c0b83a2f80eb481bbd8",
      "max": 1042301,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2ca7464c72ff472f89fe7069a7235f12",
      "value": 1042301
     }
    },
    "d959c686e5934a38853cb58240b7169d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dc3c08889fbf41549244c1146f070068": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "dfdb8b7895f34c0b83a2f80eb481bbd8": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e0ea589d0a5847d783b1ffe78e967342": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e54fafdc8ae44f04aa95f0b9cbf1a853": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5c1a76f49cda4e6da03a3fbb9870ab1c",
      "max": 548118077,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ad91d6bd84ca4b1c8b88d12d66ffae8f",
      "value": 548118077
     }
    },
    "ea26b2999ce04db0852cf13c7611b0ea": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "eacd3634757e478fa2fb4dce5c0b83d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5d3854954ae748bea3b4c8e6c6b86d77",
      "placeholder": "​",
      "style": "IPY_MODEL_9f684f3553be4a50baf6ffce2849663a",
      "value": " 1.36M/1.36M [00:00&lt;00:00, 2.34MB/s]"
     }
    },
    "efff9e496f4b4b91b27a50e5f4004694": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_36e705ee602b4f56b567b92887d31efa",
      "max": 456318,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f8f4f540c4da4705bd457fb07749fc4a",
      "value": 456318
     }
    },
    "f8f4f540c4da4705bd457fb07749fc4a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
